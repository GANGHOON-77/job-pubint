import requests
import json
import time
from datetime import datetime, timedelta
import firebase_admin
from firebase_admin import credentials, firestore
from urllib.parse import urlencode, quote
import re
from bs4 import BeautifulSoup
import logging

class PublicJobCollector:
    def __init__(self, service_key, firebase_key_path=None):
        self.service_key = service_key
        self.base_url = "http://apis.data.go.kr/1051000/recruitment/list"
        
        # Firebase ì´ˆê¸°í™” (í‚¤ íŒŒì¼ì´ ì œê³µë˜ë©´)
        self.db = None
        self.existing_job_ids = set()  # ê¸°ì¡´ ì±„ìš©ê³µê³  ID ìºì‹œ
        
        if firebase_key_path:
            try:
                cred = credentials.Certificate(firebase_key_path)
                firebase_admin.initialize_app(cred)
                self.db = firestore.client()
                print("Firebase ì—°ê²° ì„±ê³µ")
                
                # ê¸°ì¡´ ë°ì´í„° ID ë¡œë“œ
                self.load_existing_job_ids()
                
            except Exception as e:
                print(f"Firebase ì—°ê²° ì‹¤íŒ¨: {e}")
    
    def load_existing_job_ids(self):
        """ê¸°ì¡´ Firebase ë°ì´í„°ì˜ ID ëª©ë¡ì„ ë¡œë“œí•˜ì—¬ ì¤‘ë³µ ì²´í¬ìš© ìºì‹œ ìƒì„±"""
        if not self.db:
            return
        
        try:
            print("ê¸°ì¡´ ì±„ìš©ê³µê³  ID ëª©ë¡ ë¡œë“œ ì¤‘...")
            
            # Firebaseì—ì„œ ëª¨ë“  ë¬¸ì„œ IDë§Œ ê°€ì ¸ì˜¤ê¸° (ìµœì í™”)
            collection_ref = self.db.collection('recruitment_jobs')
            
            # ë°°ì¹˜ë¡œ ê°€ì ¸ì˜¤ê¸° (ì„±ëŠ¥ ìµœì í™”)
            docs = collection_ref.select(['idx', 'updated_at']).get()
            
            for doc in docs:
                self.existing_job_ids.add(doc.id)
            
            print(f"ê¸°ì¡´ ë°ì´í„° {len(self.existing_job_ids)}ê±´ ID ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.existing_job_ids = set()
    
    def is_job_exists(self, job_idx):
        """ì±„ìš©ê³µê³  IDê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        return str(job_idx) in self.existing_job_ids
    
    def add_to_cache(self, job_idx):
        """ìƒˆë¡œìš´ ì±„ìš©ê³µê³  IDë¥¼ ìºì‹œì— ì¶”ê°€"""
        self.existing_job_ids.add(str(job_idx))
    
    def needs_attachment_update(self, job_idx):
        """ê¸°ì¡´ ë°ì´í„°ì— ì²¨ë¶€íŒŒì¼ ì •ë³´ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œì§€ í™•ì¸"""
        if not self.db:
            return False
        
        try:
            doc_ref = self.db.collection('recruitment_jobs').document(str(job_idx))
            doc = doc_ref.get()
            
            if not doc.exists:
                return False
            
            data = doc.to_dict()
            attachments = data.get('attachments')
            
            # attachments í•„ë“œê°€ ì—†ê±°ë‚˜ ë¹ˆ ê²½ìš° ì—…ë°ì´íŠ¸ í•„ìš”
            if not attachments:
                return True
            
            # attachmentsê°€ ìˆì§€ë§Œ ì‹¤ì œ íŒŒì¼ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ì—…ë°ì´íŠ¸ í•„ìš”
            if isinstance(attachments, dict):
                has_files = (
                    (attachments.get('announcement') and attachments['announcement'].get('fileID')) or
                    (attachments.get('application') and attachments['application'].get('fileID')) or
                    (attachments.get('job_description') and attachments['job_description'].get('fileID')) or
                    (attachments.get('others') and len(attachments['others']) > 0)
                )
                return not has_files
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ ì²¨ë¶€íŒŒì¼ ì—…ë°ì´íŠ¸ ì²´í¬ ì‹¤íŒ¨ (idx: {job_idx}): {e}")
            return False
    
    def fetch_job_data(self, num_rows=100, page_no=1):
        """APIì—ì„œ ì±„ìš© ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        params = {
            'serviceKey': self.service_key,
            'numOfRows': str(num_rows),
            'pageNo': str(page_no),
            'returnType': 'JSON'
        }
        
        try:
            url = f"{self.base_url}?{urlencode(params)}"
            print(f"ğŸ“¡ API í˜¸ì¶œ: {url}")
            
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            # API ì‘ë‹µ êµ¬ì¡° í™•ì¸ - ìƒˆë¡œìš´ API êµ¬ì¡°
            if 'result' in data and data['result']:
                items = data['result']
                total_count = data.get('totalCount', len(items))
                return items, total_count
                    
            print("âš ï¸ ì‘ë‹µì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return [], 0
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return [], 0
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ì‘ë‹µ ë‚´ìš©: {response.text[:500]}")
            return [], 0
        except Exception as e:
            print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return [], 0
    
    def clean_and_process_job(self, job_data):
        """ì±„ìš© ë°ì´í„° ì •ë¦¬ ë° ì²˜ë¦¬"""
        try:
            # í•„ìˆ˜ í•„ë“œ ë§¤í•‘
            processed_job = {
                'idx': str(job_data.get('empmnsnIdx', '')),  # ì±„ìš©ê³µì‹œë²ˆí˜¸
                'title': str(job_data.get('empmnsnTitle', '')).strip(),  # ì±„ìš©ì œëª©
                'dept_name': str(job_data.get('deptName', '')).strip(),  # ê¸°ê´€ëª…
                'work_region': str(job_data.get('workRegion', '')).strip(),  # ê·¼ë¬´ì§€
                'employment_type': self.map_employment_type(job_data.get('empmnsnType', '')),  # ê³ ìš©í˜•íƒœ
                'reg_date': self.parse_date(job_data.get('regDate', '')),  # ë“±ë¡ì¼
                'end_date': self.parse_date(job_data.get('endDate', '')),  # ë§ˆê°ì¼
                'recruit_num': self.parse_number(job_data.get('recruitNum', '0')),  # ì±„ìš©ì¸ì›
                'recruit_type': self.map_recruit_type(job_data.get('recruitType', '')),  # ì±„ìš©í˜•íƒœ
                
                # ì¶”ê°€ ì •ë³´
                'ncs_category': str(job_data.get('ncsCategory', '')).strip(),  # NCSë¶„ë¥˜
                'education': str(job_data.get('education', '')).strip(),  # í•™ë ¥ì •ë³´
                'work_field': str(job_data.get('workField', '')).strip(),  # ê·¼ë¬´ë¶„ì•¼
                'salary_info': str(job_data.get('salaryInfo', 'íšŒì‚¬ë‚´ê·œì— ë”°ë¦„')).strip(),  # ê¸‰ì—¬ì •ë³´
                'preference': str(job_data.get('preference', '')).strip(),  # ìš°ëŒ€ì¡°ê±´
                'detail_content': str(job_data.get('detailContent', '')).strip(),  # ìƒì„¸ë‚´ìš©
                'contact_info': str(job_data.get('contactInfo', '')).strip(),  # ë¬¸ì˜ì²˜
                'attachments': self.get_job_attachments(str(job_data.get('empmnsnIdx', ''))),  # ì²¨ë¶€íŒŒì¼ ì •ë³´
                
                # ë©”íƒ€ë°ì´í„°
                'created_at': datetime.now().isoformat(),
                'updated_at': datetime.now().isoformat(),
                'status': 'active',
                'source': 'moef_api'  # ê¸°ì¬ë¶€ API
            }
            
            # ë°ì´í„° ê²€ì¦
            if not processed_job['idx'] or not processed_job['title']:
                return None
            
            # ì±„ìš©ê¸°ê°„ ìƒì„±
            if processed_job['reg_date'] and processed_job['end_date']:
                processed_job['recruit_period'] = f"{processed_job['reg_date']} ~ {processed_job['end_date']}"
            
            return processed_job
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def map_employment_type(self, emp_type):
        """ê³ ìš©í˜•íƒœ ë§¤í•‘"""
        type_map = {
            'R1010': 'ì •ê·œì§',
            'R1020': 'ê³„ì•½ì§',
            'R1030': 'ë¬´ê¸°ê³„ì•½ì§',
            'R1040': 'ë¹„ì •ê·œì§',
            'R1050': 'ì²­ë…„ì¸í„´',
            'R1060': 'ì²­ë…„ì¸í„´(ì²´í—˜í˜•)',
            'R1070': 'ì²­ë…„ì¸í„´(ì±„ìš©í˜•)',
        }
        return type_map.get(str(emp_type), str(emp_type))
    
    def map_recruit_type(self, recruit_type):
        """ì±„ìš©êµ¬ë¶„ ë§¤í•‘"""
        type_map = {
            'R2010': 'ì‹ ì…',
            'R2020': 'ê²½ë ¥',
            'R2030': 'ì‹ ì…+ê²½ë ¥',
            'R2040': 'ì™¸êµ­ì¸ ì „í˜•',
        }
        return type_map.get(str(recruit_type), str(recruit_type))
    
    def parse_date(self, date_str):
        """ë‚ ì§œ íŒŒì‹±"""
        if not date_str:
            return None
        
        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
            date_str = str(date_str).strip()
            
            # YYYY-MM-DD í˜•ì‹
            if re.match(r'\d{4}-\d{2}-\d{2}', date_str):
                return date_str[:10]
            
            # YYYYMMDD í˜•ì‹
            if re.match(r'\d{8}', date_str):
                return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
            
            # ê¸°íƒ€ í˜•ì‹ì€ í˜„ì¬ ë‚ ì§œ ë°˜í™˜
            return datetime.now().strftime('%Y-%m-%d')
            
        except Exception:
            return datetime.now().strftime('%Y-%m-%d')
    
    def parse_number(self, num_str):
        """ìˆ«ì íŒŒì‹±"""
        try:
            return int(re.sub(r'[^\d]', '', str(num_str))) if num_str else 1
        except:
            return 1
    
    def get_job_attachments(self, job_idx):
        """ì±„ìš©ê³µê³  í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ì •ë³´ í¬ë¡¤ë§"""
        if not job_idx:
            return {}
        
        try:
            # ì±„ìš©ê³µê³  ìƒì„¸ í˜ì´ì§€ URL
            detail_url = f"https://job.alio.go.kr/recruitview.do?idx={job_idx}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(detail_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ
            attachments = {
                'announcement': None,      # ê³µê³ ë¬¸ (A)
                'application': None,       # ì…ì‚¬ì§€ì›ì„œ (B) 
                'job_description': None,   # ì§ë¬´ê¸°ìˆ ì„œ (C)
                'others': [],              # ê¸°íƒ€ ì²¨ë¶€íŒŒì¼ (Z)
                'unavailable_reason': None # ë¯¸ì ‘ìˆ˜ì‚¬ìœ 
            }
            
            # ì²¨ë¶€íŒŒì¼ í…Œì´ë¸” ì°¾ê¸°
            tables = soup.find_all('table')
            for table in tables:
                if 'ì²¨ë¶€íŒŒì¼' in table.get_text() or 'ê³µê³ ë¬¸' in table.get_text():
                    rows = table.find_all('tr')
                    
                    for row in rows:
                        cells = row.find_all(['td', 'th'])
                        if len(cells) >= 2:
                            category_cell = cells[0]
                            file_cell = cells[1]
                            
                            category = category_cell.get_text(strip=True)
                            file_link = file_cell.find('a')
                            
                            if file_link:
                                href = file_link.get('href', '')
                                file_name = file_link.get_text(strip=True)
                                
                                if 'fileNo=' in href:
                                    file_id = self.extract_file_id(href)
                                    
                                    # íŒŒì¼ ìœ í˜• ë¶„ë¥˜
                                    if 'ê³µê³ ë¬¸' in category:
                                        attachments['announcement'] = {
                                            'fileID': file_id,
                                            'name': file_name,
                                            'type': 'A'
                                        }
                                    elif 'ì…ì‚¬ì§€ì›ì„œ' in category:
                                        attachments['application'] = {
                                            'fileID': file_id,
                                            'name': file_name,
                                            'type': 'B'
                                        }
                                    elif 'ì§ë¬´ê¸°ìˆ ì„œ' in category:
                                        attachments['job_description'] = {
                                            'fileID': file_id,
                                            'name': file_name,
                                            'type': 'C'
                                        }
                                    elif 'ê¸°íƒ€' in category:
                                        attachments['others'].append({
                                            'fileID': file_id,
                                            'name': file_name,
                                            'type': 'Z'
                                        })
                            elif 'ë¯¸ì ‘ìˆ˜ì‚¬ìœ ' in category and len(cells) >= 2:
                                reason = file_cell.get_text(strip=True)
                                if reason:
                                    attachments['unavailable_reason'] = reason
                    
                    break  # ì²« ë²ˆì§¸ ê´€ë ¨ í…Œì´ë¸”ë§Œ ì²˜ë¦¬
            
            return attachments
            
        except Exception as e:
            print(f"âš ï¸ ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ (idx: {job_idx}): {e}")
            return {}
    
    def extract_file_id(self, url):
        """URLì—ì„œ fileID ë˜ëŠ” fileNo ì¶”ì¶œ"""
        try:
            # fileNo= í˜•íƒœ í™•ì¸ (ìƒˆë¡œìš´ í˜•íƒœ)
            if 'fileNo=' in url:
                start = url.find('fileNo=') + 7
                end = url.find('&', start)
                if end == -1:
                    return url[start:]
                return url[start:end]
            # fileID= í˜•íƒœ í™•ì¸ (ê¸°ì¡´ í˜•íƒœ)
            elif 'fileID=' in url:
                start = url.find('fileID=') + 7
                end = url.find('&', start)
                if end == -1:
                    return url[start:]
                return url[start:end]
        except:
            pass
        return None
    
    def filter_recent_jobs(self, jobs, days=30):
        """ìµœê·¼ Nì¼ ì´ë‚´ ê³µê³ ë§Œ í•„í„°ë§"""
        if not jobs:
            return []
        
        cutoff_date = datetime.now() - timedelta(days=days)
        filtered_jobs = []
        
        for job in jobs:
            try:
                if job.get('reg_date'):
                    reg_date = datetime.strptime(job['reg_date'], '%Y-%m-%d')
                    end_date = datetime.strptime(job['end_date'], '%Y-%m-%d') if job.get('end_date') else datetime.now() + timedelta(days=30)
                    
                    # 30ì¼ ì´ë‚´ ë“±ë¡ && ë§ˆê°ì¼ì´ ì§€ë‚˜ì§€ ì•ŠìŒ
                    if reg_date >= cutoff_date and end_date >= datetime.now():
                        filtered_jobs.append(job)
            except Exception as e:
                print(f"âš ï¸ ë‚ ì§œ í•„í„°ë§ ì˜¤ë¥˜: {e}")
                continue
        
        return filtered_jobs
    
    def save_to_firebase(self, jobs):
        """Firebaseì— ë°ì´í„° ì €ì¥ (ë°°ì¹˜ ìµœì í™”)"""
        if not self.db:
            print("âŒ Firebaseê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False
        
        if not jobs:
            print("ğŸ“ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return True
        
        try:
            collection_ref = self.db.collection('recruitment_jobs')
            saved_count = 0
            updated_count = 0
            skipped_count = 0
            
            # ë°°ì¹˜ ì²˜ë¦¬ (Firestore ë°°ì¹˜ ì œí•œ: 500ê°œ)
            batch_size = 400  # ì•ˆì „í•œ í¬ê¸°ë¡œ ì„¤ì •
            
            for i in range(0, len(jobs), batch_size):
                batch_jobs = jobs[i:i + batch_size]
                batch = self.db.batch()
                
                print(f"ğŸ“¦ ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘... ({len(batch_jobs)}ê±´)")
                
                for job in batch_jobs:
                    job_idx = job['idx']
                    doc_ref = collection_ref.document(job_idx)
                    
                    # _update_attachments_only í”Œë˜ê·¸ ì œê±° (ì €ì¥ì‹œ ë¶ˆí•„ìš”)
                    update_attachments_only = job.pop('_update_attachments_only', False)
                    
                    # ì¤‘ë³µ ì²´í¬ (ìºì‹œ í™œìš©)
                    if job_idx in self.existing_job_ids:
                        if update_attachments_only:
                            # ì²¨ë¶€íŒŒì¼ ì •ë³´ë§Œ ì—…ë°ì´íŠ¸
                            update_data = {
                                'attachments': job.get('attachments'),
                                'updated_at': datetime.now().isoformat()
                            }
                            batch.update(doc_ref, update_data)
                            print(f"  ğŸ“ ì²¨ë¶€íŒŒì¼ë§Œ ì—…ë°ì´íŠ¸: {job_idx}")
                        else:
                            # ì „ì²´ ë°ì´í„° ì—…ë°ì´íŠ¸
                            job['updated_at'] = datetime.now().isoformat()
                            batch.update(doc_ref, job)
                        updated_count += 1
                    else:
                        # ìƒˆë¡œìš´ ë°ì´í„°ëŠ” ìƒì„±
                        job['created_at'] = datetime.now().isoformat()
                        job['updated_at'] = datetime.now().isoformat()
                        batch.set(doc_ref, job)
                        saved_count += 1
                        
                        # ìºì‹œì— ì¶”ê°€
                        self.existing_job_ids.add(job_idx)
                
                # ë°°ì¹˜ ì»¤ë°‹
                try:
                    batch.commit()
                    print(f"  âœ… ë°°ì¹˜ {i//batch_size + 1} ì €ì¥ ì™„ë£Œ")
                    time.sleep(0.1)  # Firebase ë¶€í•˜ ë°©ì§€
                except Exception as batch_error:
                    print(f"  âŒ ë°°ì¹˜ {i//batch_size + 1} ì €ì¥ ì‹¤íŒ¨: {batch_error}")
                    # ê°œë³„ ì €ì¥ìœ¼ë¡œ í´ë°±
                    for job in batch_jobs:
                        try:
                            doc_ref = collection_ref.document(job['idx'])
                            if job['idx'] in self.existing_job_ids:
                                doc_ref.update(job)
                                updated_count += 1
                            else:
                                doc_ref.set(job)
                                saved_count += 1
                                self.existing_job_ids.add(job['idx'])
                        except:
                            skipped_count += 1
            
            print(f"âœ… Firebase ì €ì¥ ì™„ë£Œ:")
            print(f"   - ì‹ ê·œ ì €ì¥: {saved_count}ê±´")
            print(f"   - ì—…ë°ì´íŠ¸: {updated_count}ê±´")
            if skipped_count > 0:
                print(f"   - ì‹¤íŒ¨/ìŠ¤í‚µ: {skipped_count}ê±´")
            
            return True
            
        except Exception as e:
            print(f"âŒ Firebase ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def get_collection_stats(self):
        """Firebase ì»¬ë ‰ì…˜ í†µê³„ ì¡°íšŒ"""
        if not self.db:
            return None
        
        try:
            collection_ref = self.db.collection('recruitment_jobs')
            
            # ì „ì²´ ë¬¸ì„œ ìˆ˜
            total_docs = len(list(collection_ref.stream()))
            
            # í™œì„± ìƒíƒœ ë¬¸ì„œ ìˆ˜
            active_docs = len(list(collection_ref.where('status', '==', 'active').stream()))
            
            # ìµœê·¼ ì—…ë°ì´íŠ¸ëœ ë¬¸ì„œ ìˆ˜ (ì˜¤ëŠ˜)
            today = datetime.now().date().isoformat()
            today_updated = len(list(
                collection_ref.where('updated_at', '>=', today).stream()
            ))
            
            return {
                'total': total_docs,
                'active': active_docs,
                'today_updated': today_updated
            }
            
        except Exception as e:
            print(f"âš ï¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def collect_and_save(self, max_pages=10):
        """ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ (ì¤‘ë³µ ì œê±° í¬í•¨)"""
        print("ğŸš€ ê³µê³µê¸°ê´€ ì±„ìš©ì •ë³´ ìˆ˜ì§‘ ì‹œì‘")
        print(f"ğŸ“‹ ê¸°ì¡´ ë°ì´í„° {len(self.existing_job_ids)}ê±´ í™•ì¸ë¨")
        
        all_jobs = []
        new_jobs_count = 0
        duplicate_count = 0
        page = 1
        consecutive_duplicates = 0  # ì—°ì† ì¤‘ë³µ ì¹´ìš´í„°
        
        while page <= max_pages:
            print(f"\nğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
            
            jobs_data, total_count = self.fetch_job_data(num_rows=100, page_no=page)
            
            if not jobs_data:
                print("ğŸ“­ ë” ì´ìƒ ê°€ì ¸ì˜¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                break
            
            # ë°ì´í„° ì²˜ë¦¬ ë° ì¤‘ë³µ ì²´í¬
            processed_jobs = []
            page_new_count = 0
            page_duplicate_count = 0
            
            for job_data in jobs_data:
                # ê¸°ë³¸ ì²˜ë¦¬
                processed_job = self.clean_and_process_job(job_data)
                if not processed_job:
                    continue
                
                job_idx = processed_job['idx']
                
                # ì¤‘ë³µ ì²´í¬ ë° ì²¨ë¶€íŒŒì¼ ì •ë³´ ì—…ë°ì´íŠ¸ ì²´í¬
                if self.is_job_exists(job_idx):
                    # ê¸°ì¡´ ë°ì´í„°ì— ì²¨ë¶€íŒŒì¼ ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸
                    needs_attachment_update = self.needs_attachment_update(job_idx)
                    
                    if not needs_attachment_update:
                        duplicate_count += 1
                        page_duplicate_count += 1
                        print(f"  âš ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {job_idx} - {processed_job.get('title', 'No Title')[:30]}...")
                        continue
                    else:
                        print(f"  ğŸ”„ ì²¨ë¶€íŒŒì¼ ì—…ë°ì´íŠ¸: {job_idx} - {processed_job.get('title', 'No Title')[:30]}...")
                        # ì²¨ë¶€íŒŒì¼ ì •ë³´ë§Œ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²½ìš°ë¡œ í‘œì‹œ
                        processed_job['_update_attachments_only'] = True
                
                # ìƒˆë¡œìš´ ë°ì´í„°
                processed_jobs.append(processed_job)
                new_jobs_count += 1
                page_new_count += 1
                
                # ìºì‹œì— ì¶”ê°€ (ë‹¤ìŒ í˜ì´ì§€ì—ì„œ ì¤‘ë³µ ì²´í¬í•  ìˆ˜ ìˆë„ë¡)
                self.add_to_cache(job_idx)
            
            print(f"âœ… í˜ì´ì§€ {page} ê²°ê³¼: ì‹ ê·œ {page_new_count}ê±´, ì¤‘ë³µ {page_duplicate_count}ê±´")
            
            # ì „ì²´ í˜ì´ì§€ê°€ ì¤‘ë³µì´ë©´ ìˆ˜ì§‘ ì¤‘ë‹¨ (íš¨ìœ¨ì„±)
            if page_duplicate_count == len(jobs_data) and len(jobs_data) > 0:
                consecutive_duplicates += 1
                print(f"âš ï¸ í˜ì´ì§€ ì „ì²´ ì¤‘ë³µ ({consecutive_duplicates}ë²ˆì§¸)")
                
                # ì—°ì† 3í˜ì´ì§€ê°€ ëª¨ë‘ ì¤‘ë³µì´ë©´ ìˆ˜ì§‘ ì¤‘ë‹¨
                if consecutive_duplicates >= 3:
                    print("ğŸ›‘ ì—°ì† ì¤‘ë³µ í˜ì´ì§€ê°€ ë§ì•„ ìˆ˜ì§‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤")
                    break
            else:
                consecutive_duplicates = 0  # ì¤‘ë³µì´ ì•„ë‹ˆë©´ ì¹´ìš´í„° ë¦¬ì…‹
            
            all_jobs.extend(processed_jobs)
            
            # ì‹ ê·œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë‹¤ìŒ í˜ì´ì§€ë¡œ ë¹ ë¥´ê²Œ ì´ë™
            if page_new_count == 0:
                time.sleep(0.5)  # ì§§ì€ ëŒ€ê¸°
            else:
                time.sleep(1)    # ì •ìƒ ëŒ€ê¸°
                
            page += 1
        
        print(f"\nğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ í†µê³„:")
        print(f"   - ì „ì²´ ì²˜ë¦¬: {new_jobs_count + duplicate_count}ê±´")
        print(f"   - ì‹ ê·œ ë°ì´í„°: {new_jobs_count}ê±´")
        print(f"   - ì¤‘ë³µ ìŠ¤í‚µ: {duplicate_count}ê±´")
        
        if all_jobs:
            # ìµœê·¼ 30ì¼ ë°ì´í„°ë§Œ í•„í„°ë§
            filtered_jobs = self.filter_recent_jobs(all_jobs, days=30)
            print(f"ğŸ” ë‚ ì§œ í•„í„°ë§: {len(filtered_jobs)}ê±´ (ì „ì²´ {len(all_jobs)}ê±´)")
            
            if filtered_jobs:
                # Firebaseì— ì €ì¥
                if self.db:
                    self.save_to_firebase(filtered_jobs)
                
                # JSON íŒŒì¼ë¡œë„ ë°±ì—… ì €ì¥
                self.save_to_json(filtered_jobs)
            
            return filtered_jobs
        
        print("ğŸ“ ì‹ ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return []
    
    def save_to_json(self, jobs, filename=None):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            filename = f"recruitment_jobs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({
                    'collected_at': datetime.now().isoformat(),
                    'total_count': len(jobs),
                    'jobs': jobs
                }, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            
        except Exception as e:
            print(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    # API í‚¤
    SERVICE_KEY = "1bmDITdGFoaDTSrbT6Uyz8bFdlIL3nydHgRu0xQtXO8SiHlCrOJKv+JNSythF12BiijhVB3qE96/4Jxr70zUNg=="
    
    # Firebase í‚¤ íŒŒì¼ ê²½ë¡œ
    FIREBASE_KEY_PATH = "info-gov-firebase-adminsdk-9o52l-f6b59e8ae8.json"
    
    # ì»¬ë ‰í„° ì´ˆê¸°í™”
    collector = PublicJobCollector(SERVICE_KEY, FIREBASE_KEY_PATH)
    
    # ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥
    jobs = collector.collect_and_save(max_pages=5)
    
    print(f"\nğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(jobs)}ê±´ì˜ ì±„ìš© ì •ë³´ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()