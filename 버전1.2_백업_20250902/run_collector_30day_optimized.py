# -*- coding: utf-8 -*-
import sys
import os
from datetime import datetime, timedelta
sys.stdout.reconfigure(encoding='utf-8')

from data_collector import PublicJobCollector

def collect_30day_optimized_data():
    """30ì¼ ì´ë‚´ ë“±ë¡, ì§„í–‰ì¤‘ì¸ ê²Œì‹œê¸€ë§Œ ìµœì í™” ìˆ˜ì§‘ (ì¤‘ë³µ ìŠ¤í‚µ + ì²¨ë¶€íŒŒì¼ í¬í•¨)"""
    # API í‚¤
    SERVICE_KEY = "1bmDITdGFoaDTSrbT6Uyz8bFdlIL3nydHgRu0xQtXO8SiHlCrOJKv+JNSythF12BiijhVB3qE96/4Jxr70zUNg=="
    
    # Firebase í‚¤ íŒŒì¼ ê²½ë¡œ
    FIREBASE_KEY_PATH = "job-pubint-firebase-adminsdk-fbsvc-8a7f28a86e.json"
    
    print("=== 30ì¼ ì´ë‚´ ì§„í–‰ì¤‘ ì±„ìš©ê³µê³  ìµœì í™” ìˆ˜ì§‘ê¸° ì‹œì‘ ===")
    print("âœ¨ ê°œì„ ì‚¬í•­: ì¤‘ë³µ ìŠ¤í‚µ + ì²¨ë¶€íŒŒì¼ í¬í•¨ + API í˜¸ì¶œ ìµœì†Œí™”")
    
    # 30ì¼ ì „ ë‚ ì§œ ê³„ì‚°
    cutoff_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
    print(f"ğŸ—“ï¸  ìˆ˜ì§‘ ê¸°ì¤€ì¼: {cutoff_date} ì´í›„ ë“±ë¡")
    
    # ì»¬ë ‰í„° ì´ˆê¸°í™”
    collector = PublicJobCollector(SERVICE_KEY, FIREBASE_KEY_PATH)
    
    print(f"ğŸ“Š ê¸°ì¡´ ë°ì´í„°: {len(collector.existing_job_ids)}ê±´ í™•ì¸ë¨ (ì¤‘ë³µ ì²´í¬ ì¤€ë¹„)")
    
    # ìˆ˜ì§‘ í†µê³„
    stats = {
        'total_processed': 0,
        'new_saved': 0,
        'duplicates_skipped': 0,
        'attachment_included': 0,
        'api_calls': 0
    }
    
    all_jobs = []
    
    try:
        # API í˜¸ì¶œ ìµœì í™”: í˜ì´ì§€ë³„ ì²˜ë¦¬
        for page in range(1, 11):  # 10í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘ (1000ê±´)
            print(f"\nğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
            
            # API í˜¸ì¶œ
            stats['api_calls'] += 1
            jobs_data, total_count = collector.fetch_job_data(num_rows=100, page_no=page)
            
            if not jobs_data:
                print(f"   âŒ í˜ì´ì§€ {page}: ë°ì´í„° ì—†ìŒ - ìˆ˜ì§‘ ì¢…ë£Œ")
                break
            
            print(f"   ğŸ“¦ í˜ì´ì§€ {page}: {len(jobs_data)}ê±´ API ë°ì´í„° ìˆ˜ì‹ ")
            
            page_new = 0
            page_duplicates = 0
            page_filtered = 0
            
            # ê° ë°ì´í„° ì²˜ë¦¬
            for job_data in jobs_data:
                stats['total_processed'] += 1
                
                # 1ë‹¨ê³„: ì§„í–‰ì¤‘ì¸ ê²Œì‹œê¸€ë§Œ í•„í„°ë§ (ongoingYn = 'Y')
                if job_data.get('ongoingYn') != 'Y':
                    page_filtered += 1
                    continue
                
                # 2ë‹¨ê³„: ì¤‘ë³µ ì²´í¬ (API í˜¸ì¶œ ìµœì†Œí™”)
                job_idx = str(job_data.get('recrutPblntSn', ''))
                if collector.is_job_exists(job_idx):
                    page_duplicates += 1
                    stats['duplicates_skipped'] += 1
                    continue
                
                # 3ë‹¨ê³„: ë°ì´í„° ì •ì œ ë° ì²˜ë¦¬
                processed_job = collector.clean_and_process_job(job_data)
                if not processed_job:
                    continue
                
                # 4ë‹¨ê³„: 30ì¼ ì´ë‚´ ë“±ë¡ í•„í„°
                reg_date = processed_job.get('reg_date', '')
                if not reg_date or reg_date < cutoff_date:
                    continue
                
                # 5ë‹¨ê³„: ì²¨ë¶€íŒŒì¼ ì •ë³´ í¬í•¨ í™•ì¸
                attachments = processed_job.get('attachments', {})
                has_attachments = bool(attachments and (
                    attachments.get('announcement') or 
                    attachments.get('application') or 
                    attachments.get('job_description') or 
                    attachments.get('others')
                ))
                
                if has_attachments:
                    stats['attachment_included'] += 1
                
                # 6ë‹¨ê³„: Firebase ì €ì¥ (ì‹ ê·œë§Œ)
                try:
                    if collector.db:
                        doc_id = str(processed_job['idx'])
                        
                        # Firebaseì— ì €ì¥
                        collector.db.collection('recruitment_jobs').document(doc_id).set(processed_job)
                        collector.add_to_cache(doc_id)  # ìºì‹œ ì—…ë°ì´íŠ¸
                        all_jobs.append(processed_job)
                        
                        page_new += 1
                        stats['new_saved'] += 1
                        
                        attachment_status = "ğŸ“" if has_attachments else "ğŸ“‹"
                        print(f"   âœ… ì €ì¥: {processed_job['dept_name'][:15]} - {processed_job['title'][:30]}... ({reg_date}) {attachment_status}")
                    
                except Exception as e:
                    print(f"   âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
            
            # í˜ì´ì§€ë³„ ìš”ì•½
            print(f"   ğŸ“Š í˜ì´ì§€ {page} ìš”ì•½: ì‹ ê·œ {page_new}ê±´ | ì¤‘ë³µ {page_duplicates}ê±´ | í•„í„°ì œì™¸ {page_filtered}ê±´")
            
            # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ (ìµœì í™”)
            if page_new == 0 and page_duplicates == 0:
                print(f"   ğŸ”š ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ì–´ ìˆ˜ì§‘ ì¢…ë£Œ")
                break
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*60}")
        print(f"ğŸ¯ ìµœì¢… ìˆ˜ì§‘ ê²°ê³¼")
        print(f"{'='*60}")
        print(f"ğŸ“‹ ì´ ì²˜ë¦¬ëœ ë°ì´í„°: {stats['total_processed']:,}ê±´")
        print(f"âœ… ì‹ ê·œ ì €ì¥: {stats['new_saved']:,}ê±´")
        print(f"â­ï¸  ì¤‘ë³µ ìŠ¤í‚µ: {stats['duplicates_skipped']:,}ê±´")
        print(f"ğŸ“ ì²¨ë¶€íŒŒì¼ í¬í•¨: {stats['attachment_included']:,}ê±´")
        print(f"ğŸ”„ API í˜¸ì¶œ ìˆ˜: {stats['api_calls']:,}íšŒ")
        
        if stats['new_saved'] > 0:
            print(f"ğŸ“ˆ ì²¨ë¶€íŒŒì¼ í¬í•¨ìœ¨: {(stats['attachment_included']/stats['new_saved']*100):.1f}%")
        
        # ìƒì„¸ í†µê³„
        if all_jobs:
            print(f"\nğŸ“Š ìƒì„¸ ë¶„ì„:")
            
            # ë‚ ì§œë³„ í†µê³„
            date_count = {}
            for job in all_jobs:
                date = job.get('reg_date', 'ì•Œ ìˆ˜ ì—†ìŒ')
                date_count[date] = date_count.get(date, 0) + 1
            
            print(f"\nğŸ—“ï¸  ë‚ ì§œë³„ ì‹ ê·œ ë°ì´í„° (ìµœê·¼ 10ì¼):")
            for date, count in sorted(date_count.items(), reverse=True)[:10]:
                print(f"   - {date}: {count}ê±´")
            
            # ê¸°ê´€ë³„ í†µê³„
            inst_count = {}
            for job in all_jobs:
                inst = job.get('dept_name', 'ì•Œ ìˆ˜ ì—†ìŒ')
                inst_count[inst] = inst_count.get(inst, 0) + 1
            
            print(f"\nğŸ¢ ê¸°ê´€ë³„ ì‹ ê·œ ë°ì´í„° (ìƒìœ„ 10ê°œ):")
            for inst, count in sorted(inst_count.items(), key=lambda x: x[1], reverse=True)[:10]:
                print(f"   - {inst}: {count}ê±´")
            
            # ê³ ìš©í˜•íƒœë³„ í†µê³„
            emp_count = {}
            for job in all_jobs:
                emp_type = job.get('employment_type', 'ì•Œ ìˆ˜ ì—†ìŒ')
                emp_count[emp_type] = emp_count.get(emp_type, 0) + 1
            
            print(f"\nğŸ’¼ ê³ ìš©í˜•íƒœë³„ ì‹ ê·œ ë°ì´í„°:")
            for emp_type, count in sorted(emp_count.items(), key=lambda x: x[1], reverse=True):
                print(f"   - {emp_type}: {count}ê±´")
                
    except Exception as e:
        print(f"ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False, stats
    
    return True, stats

if __name__ == "__main__":
    success, final_stats = collect_30day_optimized_data()
    if success:
        print(f"\nğŸ‰ 30ì¼ ì´ë‚´ ìµœì í™” ìˆ˜ì§‘ê¸° ì‹¤í–‰ ì™„ë£Œ")
        print(f"ğŸ’¡ íš¨ìœ¨ì„±: ì´ {final_stats['api_calls']}ë²ˆì˜ API í˜¸ì¶œë¡œ {final_stats['new_saved']}ê±´ ì‹ ê·œ ìˆ˜ì§‘")
    else:
        print(f"\nâŒ 30ì¼ ì´ë‚´ ìµœì í™” ìˆ˜ì§‘ê¸° ì‹¤í–‰ ì‹¤íŒ¨")