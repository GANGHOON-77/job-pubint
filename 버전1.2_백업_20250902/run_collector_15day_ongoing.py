# -*- coding: utf-8 -*-
import sys
import os
sys.stdout.reconfigure(encoding='utf-8')

from data_collector import PublicJobCollector

def collect_15day_ongoing_data():
    """15ì¼ ì´ë‚´ ë“±ë¡, ì§„í–‰ì¤‘ì¸ ê²Œì‹œê¸€ë§Œ ìˆ˜ì§‘"""
    # API í‚¤
    SERVICE_KEY = "1bmDITdGFoaDTSrbT6Uyz8bFdlIL3nydHgRu0xQtXO8SiHlCrOJKv+JNSythF12BiijhVB3qE96/4Jxr70zUNg=="
    
    # Firebase í‚¤ íŒŒì¼ ê²½ë¡œ
    FIREBASE_KEY_PATH = "job-pubint-firebase-adminsdk-fbsvc-1c4c2dbd08.json"
    
    print("=== 15ì¼ ì´ë‚´ ì§„í–‰ì¤‘ ì±„ìš©ê³µê³  ìˆ˜ì§‘ê¸° ì‹œì‘ ===")
    
    # ì»¬ë ‰í„° ì´ˆê¸°í™”
    collector = PublicJobCollector(SERVICE_KEY, FIREBASE_KEY_PATH)
    
    print(f"ê¸°ì¡´ ë°ì´í„°: {len(collector.existing_job_ids)}ê±´ í™•ì¸ë¨")
    
    # ì§ì ‘ API í˜¸ì¶œí•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
    all_jobs = []
    
    try:
        for page in range(1, 6):  # 5í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘ (500ê±´)
            print(f"\nğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘...")
            
            jobs_data, total_count = collector.fetch_job_data(num_rows=100, page_no=page)
            
            if not jobs_data:
                print(f"í˜ì´ì§€ {page}: ë°ì´í„° ì—†ìŒ")
                continue
            
            print(f"í˜ì´ì§€ {page}: {len(jobs_data)}ê±´ ë°ì´í„° í™•ì¸")
            
            # ê° ë°ì´í„° ì²˜ë¦¬
            for job_data in jobs_data:
                # ì§„í–‰ì¤‘ì¸ ê²Œì‹œê¸€ë§Œ í•„í„°ë§ (ongoingYn = 'Y')
                if job_data.get('ongoingYn') != 'Y':
                    continue
                
                processed_job = collector.clean_and_process_job(job_data)
                if processed_job:
                    # 15ì¼ ì´ë‚´ ë“±ë¡ í•„í„° (2025-08-17 ì´í›„)
                    reg_date = processed_job.get('reg_date', '')
                    if reg_date and reg_date >= '2025-08-17':
                        try:
                            if collector.db:
                                doc_id = str(processed_job['idx'])
                                
                                # Firebaseì— ì €ì¥
                                collector.db.collection('recruitment_jobs').document(doc_id).set(processed_job)
                                collector.add_to_cache(doc_id)
                                all_jobs.append(processed_job)
                                
                                print(f"âœ… ì €ì¥: {processed_job['dept_name']} - {processed_job['title'][:40]}... ({reg_date})")
                            
                        except Exception as e:
                            print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
                    else:
                        print(f"  ğŸ” ë‚ ì§œ í•„í„°ë§ ì œì™¸: {reg_date}")
                        
        print(f"\n=== ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(all_jobs)}ê±´ ì €ì¥ë¨ ===")
        
        # í†µê³„ ì¶œë ¥
        if all_jobs:
            # ë‚ ì§œë³„ í†µê³„
            date_count = {}
            for job in all_jobs:
                date = job.get('reg_date', 'ì•Œ ìˆ˜ ì—†ìŒ')
                date_count[date] = date_count.get(date, 0) + 1
            
            print(f"\nğŸ“Š ë‚ ì§œë³„ ì €ì¥ í†µê³„:")
            for date, count in sorted(date_count.items(), reverse=True)[:10]:
                print(f"   - {date}: {count}ê±´")
            
            # ê¸°ê´€ë³„ í†µê³„
            inst_count = {}
            for job in all_jobs:
                inst = job.get('dept_name', 'ì•Œ ìˆ˜ ì—†ìŒ')
                inst_count[inst] = inst_count.get(inst, 0) + 1
            
            print(f"\nğŸ¢ ê¸°ê´€ë³„ ì €ì¥ í†µê³„ (ìƒìœ„ 10ê°œ):")
            for inst, count in sorted(inst_count.items(), key=lambda x: x[1], reverse=True)[:10]:
                print(f"   - {inst}: {count}ê±´")
                
    except Exception as e:
        print(f"ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = collect_15day_ongoing_data()
    if success:
        print("\nğŸ‰ 15ì¼ ì´ë‚´ ì§„í–‰ì¤‘ ìˆ˜ì§‘ê¸° ì‹¤í–‰ ì™„ë£Œ")
    else:
        print("\nâŒ 15ì¼ ì´ë‚´ ì§„í–‰ì¤‘ ìˆ˜ì§‘ê¸° ì‹¤í–‰ ì‹¤íŒ¨")